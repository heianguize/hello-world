{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl \n",
    "# !pip install numpy matplotlib spacy torchtext seaborn\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math,copy,time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set_context(context=\"talk\") \n",
    "# seaborn只在最后可视化self-attention的时候用到，\n",
    "# 可以先不管或者注释掉这两行\n",
    "%matplotlib inline \n",
    "# only for jupyter notebook\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "  def __init__(self,d_model,vocab):\n",
    "    #d_model=512, vocab=当前语言的词表大小\n",
    "    super(Embeddings,self).__init__()\n",
    "    self.lut=nn.Embedding(vocab,d_model) \n",
    "    # one-hot转词嵌入，这里有一个待训练的矩阵E，大小是vocab*d_model\n",
    "    self.d_model=d_model # 512\n",
    "  def forward(self,x): \n",
    "     # x ~ (batch.size, sequence.length, one-hot), \n",
    "     #one-hot大小=vocab，当前语言的词表大小\n",
    "     return self.lut(x)*math.sqrt(self.d_model) \n",
    "     # 得到的10*512词嵌入矩阵，主动乘以sqrt(512)=22.6，\n",
    "     #这里我做了一些对比，感觉这个乘以sqrt(512)没啥用… 求反驳。\n",
    "     #这里的输出的tensor大小类似于(batch.size, sequence.length, 512)\n",
    "  \n",
    "class PositionalEncoding(nn.Module): \n",
    "    \"Implement the PE function.\" \n",
    "    def __init__(self, d_model, dropout, max_len=5000): \n",
    "        #d_model=512,dropout=0.1,\n",
    "        #max_len=5000代表事先准备好长度为5000的序列的位置编码，其实没必要，\n",
    "        #一般100或者200足够了。\n",
    "        super(PositionalEncoding, self).__init__() \n",
    "        self.dropout = nn.Dropout(p=dropout) \n",
    "\n",
    "        # Compute the positional encodings once in log space. \n",
    "        pe = torch.zeros(max_len, d_model) \n",
    "        #(5000,512)矩阵，保持每个位置的位置编码，一共5000个位置，\n",
    "        #每个位置用一个512维度向量来表示其位置编码\n",
    "        position = torch.arange(0, max_len).unsqueeze(1) \n",
    "        # (5000) -> (5000,1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *  \\\n",
    "        -(math.log(10000.0) / d_model)) \n",
    "        # (0,2,…, 4998)一共准备2500个值，供sin, cos调用\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # 偶数下标的位置\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # 奇数下标的位置\n",
    "        pe = pe.unsqueeze(0) \n",
    "        # (5000, 512) -> (1, 5000, 512) 为batch.size留出位置\n",
    "        self.register_buffer('pe', pe) \n",
    "    def forward(self, x): \n",
    "        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False) \n",
    "        # 接受1.Embeddings的词嵌入结果x，\n",
    "        #然后把自己的位置编码pe，封装成torch的Variable(不需要梯度)，加上去。\n",
    "        #例如，假设x是(30,10,512)的一个tensor，\n",
    "        #30是batch.size, 10是该batch的序列长度, 512是每个词的词嵌入向量；\n",
    "        #则该行代码的第二项是(1, min(10, 5000), 512)=(1,10,512)，\n",
    "        #在具体相加的时候，会扩展(1,10,512)为(30,10,512)，\n",
    "        #保证一个batch中的30个序列，都使用（叠加）一样的位置编码。\n",
    "        return self.dropout(x) # 增加一次dropout操作\n",
    "    # 注意，位置编码不会更新，是写死的，所以这个class里面没有可训练的参数。\n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None): \n",
    "    # query, key, value的形状类似于(30, 8, 10, 64), (30, 8, 11, 64), \n",
    "    #(30, 8, 11, 64)，例如30是batch.size，即当前batch中有多少一个序列；\n",
    "    # 8=head.num，注意力头的个数；\n",
    "    # 10=目标序列中词的个数，64是每个词对应的向量表示；\n",
    "    # 11=源语言序列传过来的memory中，当前序列的词的个数，\n",
    "    # 64是每个词对应的向量表示。\n",
    "    # 类似于，这里假定query来自target language sequence；\n",
    "    # key和value都来自source language sequence.\n",
    "    \"Compute 'Scaled Dot Product Attention'\" \n",
    "    d_k = query.size(-1) # 64=d_k\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / \\\n",
    "        math.sqrt(d_k) # 先是(30,8,10,64)和(30, 8, 64, 11)相乘，\n",
    "        #（注意是最后两个维度相乘）得到(30,8,10,11)，\n",
    "        #代表10个目标语言序列中每个词和11个源语言序列的分别的“亲密度”。\n",
    "        #然后除以sqrt(d_k)=8，防止过大的亲密度。\n",
    "        #这里的scores的shape是(30, 8, 10, 11)\n",
    "    if mask is not None: \n",
    "        scores = scores.masked_fill(mask == 0, -1e9) \n",
    "        #使用mask，对已经计算好的scores，按照mask矩阵，填-1e9，\n",
    "        #然后在下一步计算softmax的时候，被设置成-1e9的数对应的值~0,被忽视\n",
    "    p_attn = F.softmax(scores, dim = -1) \n",
    "        #对scores的最后一个维度执行softmax，得到的还是一个tensor, \n",
    "        #(30, 8, 10, 11)\n",
    "    if dropout is not None: \n",
    "        p_attn = dropout(p_attn) #执行一次dropout\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "    #返回的第一项，是(30,8,10, 11)乘以（最后两个维度相乘）\n",
    "    #value=(30,8,11,64)，得到的tensor是(30,8,10,64)，\n",
    "    #和query的最初的形状一样。另外，返回p_attn，形状为(30,8,10,11). \n",
    "    #注意，这里返回p_attn主要是用来可视化显示多头注意力机制。\n",
    "\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers.\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
    "\n",
    "class MultiHeadedAttention(nn.Module): \n",
    "  def __init__(self, h, d_model, dropout=0.1): \n",
    "    # h=8, d_model=512\n",
    "    \"Take in model size and number of heads.\" \n",
    "    super(MultiHeadedAttention, self).__init__() \n",
    "    assert d_model % h == 0 # We assume d_v always equals d_k 512%8=0\n",
    "    self.d_k = d_model // h # d_k=512//8=64\n",
    "    self.h = h #8\n",
    "    self.linears = clones(nn.Linear(d_model, d_model), 4) \n",
    "    #定义四个Linear networks, 每个的大小是(512, 512)的，\n",
    "    #每个Linear network里面有两类可训练参数，Weights，\n",
    "    #其大小为512*512，以及biases，其大小为512=d_model。\n",
    "\n",
    "    self.attn = None \n",
    "    self.dropout = nn.Dropout(p=dropout)\n",
    "  def forward(self, query, key, value, mask=None): \n",
    "   # 注意，输入query的形状类似于(30, 10, 512)，\n",
    "   # key.size() ~ (30, 11, 512), \n",
    "   #以及value.size() ~ (30, 11, 512)\n",
    "    \n",
    "    if mask is not None: # Same mask applied to all h heads. \n",
    "      mask = mask.unsqueeze(1) # mask下回细细分解。\n",
    "    nbatches = query.size(0) #e.g., nbatches=30\n",
    "    # 1) Do all the linear projections in batch from \n",
    "    #d_model => h x d_k \n",
    "    query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k)\n",
    "      .transpose(1, 2) for l, x in \n",
    "      zip(self.linears, (query, key, value))] \n",
    "      # 这里是前三个Linear Networks的具体应用，\n",
    "      #例如query=(30,10, 512) -> Linear network -> (30, 10, 512) \n",
    "      #-> view -> (30,10, 8, 64) -> transpose(1,2) -> (30, 8, 10, 64)\n",
    "      #，其他的key和value也是类似地，\n",
    "      #从(30, 11, 512) -> (30, 8, 11, 64)。\n",
    "    # 2) Apply attention on all the projected vectors in batch. \n",
    "    x, self.attn = attention(query, key, value, mask=mask, \n",
    "      dropout=self.dropout) \n",
    "      #调用上面定义好的attention函数，输出的x形状为(30, 8, 10, 64)；\n",
    "      #attn的形状为(30, 8, 10=target.seq.len, 11=src.seq.len)\n",
    "    # 3) \"Concat\" using a view and apply a final linear. \n",
    "    x = x.transpose(1, 2).contiguous(). \\\n",
    "      view(nbatches, -1, self.h * self.d_k) \n",
    "      # x ~ (30, 8, 10, 64) -> transpose(1,2) -> \n",
    "      #(30, 10, 8, 64) -> contiguous() and view -> \n",
    "      #(30, 10, 8*64) = (30, 10, 512)\n",
    "    return self.linears[-1](x) \n",
    "#执行第四个Linear network，把(30, 10, 512)经过一次linear network，\n",
    "#得到(30, 10, 512).\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        # features=d_model=512, eps=epsilon 用于分母的非0化平滑\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        # a_2 是一个可训练参数向量，(512)\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        # b_2 也是一个可训练参数向量, (512)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x 的形状为(batch.size, sequence.len, 512)\n",
    "        mean = x.mean(-1, keepdim=True) \n",
    "        # 对x的最后一个维度，取平均值，得到tensor (batch.size, seq.len)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        # 对x的最后一个维度，取标准方差，得(batch.size, seq.len)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "        # 本质上类似于（x-mean)/std，不过这里加入了两个可训练向量\n",
    "        # a_2 and b_2，以及分母上增加一个极小值epsilon，用来防止std为0\n",
    "        # 的时候的除法溢出\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        # size=d_model=512; dropout=0.1\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size) # (512)，用来定义a_2和b_2\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the \"\n",
    "        \"same size.\"\n",
    "        # x is alike (batch.size, sequence.len, 512)\n",
    "        # sublayer是一个具体的MultiHeadAttention\n",
    "        #或者PositionwiseFeedForward对象\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "        # x (30, 10, 512) -> norm (LayerNorm) -> (30, 10, 512)\n",
    "        # -> sublayer (MultiHeadAttention or PositionwiseFeedForward)\n",
    "        # -> (30, 10, 512) -> dropout -> (30, 10, 512)\n",
    "        \n",
    "        # 然后输入的x（没有走sublayer) + 上面的结果，\n",
    "        #即实现了残差相加的功能\n",
    "    \n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        # d_model = 512\n",
    "        # d_ff = 2048 = 512*4\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        # 构建第一个全连接层，(512, 2048)，其中有两种可训练参数：\n",
    "        # weights矩阵，(512, 2048)，以及\n",
    "        # biases偏移向量, (2048)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        # 构建第二个全连接层, (2048, 512)，两种可训练参数：\n",
    "        # weights矩阵，(2048, 512)，以及\n",
    "        # biases偏移向量, (512)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape = (batch.size, sequence.len, 512)\n",
    "        # 例如, (30, 10, 512)\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "        # x (30, 10, 512) -> self.w_1 -> (30, 10, 2048)\n",
    "        # -> relu -> (30, 10, 2048) \n",
    "        # -> dropout -> (30, 10, 2048)\n",
    "        # -> self.w_2 -> (30, 10, 512)是输出的shape\n",
    "    \n",
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and \"\n",
    "    \"feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        # size=d_model=512\n",
    "        # self_attn = MultiHeadAttention对象, first sublayer\n",
    "        # feed_forward = PositionwiseFeedForward对象，second sublayer\n",
    "        # dropout = 0.1 (e.g.)\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        # 使用深度克隆方法，完整地复制出来两个SublayerConnection\n",
    "        self.size = size # 512\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Follow Figure 1 (left) for connections.\"\n",
    "        # x shape = (30, 10, 512)\n",
    "        # mask 是(batch.size, 10,10)的矩阵，类似于当前一个词w，有哪些词是w可见的\n",
    "        # 源语言的序列的话，所有其他词都可见，除了\"<blank>\"这样的填充；\n",
    "        # 目标语言的序列的话，所有w的左边的词，都可见。\n",
    "        x = self.sublayer[0](x, \n",
    "          lambda x: self.self_attn(x, x, x, mask))\n",
    "        # x (30, 10, 512) -> self_attn (MultiHeadAttention) \n",
    "        # shape is same (30, 10, 512) -> SublayerConnection \n",
    "        # -> (30, 10, 512)\n",
    "        return self.sublayer[1](x, self.feed_forward)\n",
    "        # x 和feed_forward对象一起，给第二个SublayerConnection\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer, N):\n",
    "        # layer = one EncoderLayer object, N=6\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N) \n",
    "        # 深copy，N=6，\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        # 定义一个LayerNorm，layer.size=d_model=512\n",
    "        # 其中有两个可训练参数a_2和b_2\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn.\"\n",
    "        # x is alike (30, 10, 512)\n",
    "        # (batch.size, sequence.len, d_model)\n",
    "        # mask是类似于(batch.size, 10, 10)的矩阵\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "            # 进行六次EncoderLayer操作\n",
    "        return self.norm(x)\n",
    "        # 最后做一次LayerNorm，最后的输出也是(30, 10, 512) shape\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made of self-attn, src-attn, \"\n",
    "    \"and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, src_attn, \n",
    "      feed_forward, dropout):\n",
    "      # size = d_model=512,\n",
    "      # self_attn = one MultiHeadAttention object，目标语言序列的\n",
    "      # src_attn = second MultiHeadAttention object, 目标语言序列\n",
    "      # 和源语言序列之间的\n",
    "      # feed_forward 一个全连接层\n",
    "      # dropout = 0.1\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size # 512\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "        # 需要三个SublayerConnection, 分别在\n",
    "        # self.self_attn, self.src_attn, 和self.feed_forward\n",
    "        # 的后边\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"Follow Figure 1 (right) for connections.\"\n",
    "        m = memory # (batch.size, sequence.len, 512) \n",
    "        # 来自源语言序列的Encoder之后的输出，作为memory\n",
    "        # 供目标语言的序列检索匹配：（类似于alignment in SMT)\n",
    "        x = self.sublayer[0](x, \n",
    "          lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        # 通过一个匿名函数，来实现目标序列的自注意力编码\n",
    "        # 结果扔给sublayer[0]:SublayerConnection\n",
    "        x = self.sublayer[1](x, \n",
    "          lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        # 通过第二个匿名函数，来实现目标序列和源序列的注意力计算\n",
    "        # 结果扔给sublayer[1]:SublayerConnection\n",
    "        return self.sublayer[2](x, self.feed_forward)\n",
    "        # 走一个全连接层，然后\n",
    "        # 结果扔给sublayer[2]:SublayerConnection\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "    def __init__(self, layer, N):\n",
    "        # layer = DecoderLayer object\n",
    "        # N = 6\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        # 深度copy六次DecoderLayer\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        # 初始化一个LayerNorm\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "            # 执行六次DecoderLayer\n",
    "        return self.norm(x)\n",
    "        # 执行一次LayerNorm\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        # d_model=512\n",
    "        # vocab = 目标语言词表大小\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "        # 定义一个全连接层，可训练参数个数是(512 * trg_vocab_size) + \n",
    "        # trg_vocab_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)\n",
    "        # x 类似于 (batch.size, sequence.length, 512)\n",
    "        # -> proj 全连接层 (30, 10, trg_vocab_size) = logits\n",
    "        # 对最后一个维度执行log_soft_max\n",
    "        # 得到(30, 10, trg_vocab_size)\n",
    "    \n",
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture. \n",
    "    Base for this and many other models.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder, decoder, \n",
    "      src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        # Encoder对象\n",
    "        self.decoder = decoder\n",
    "        # Decoder对象\n",
    "        self.src_embed = src_embed\n",
    "        # 源语言序列的编码，包括词嵌入和位置编码\n",
    "        self.tgt_embed = tgt_embed\n",
    "        # 目标语言序列的编码，包括词嵌入和位置编码\n",
    "        self.generator = generator\n",
    "        # 生成器\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        return self.decode(self.encode(src, src_mask), src_mask,\n",
    "                            tgt, tgt_mask)\n",
    "        # 先对源语言序列进行编码，\n",
    "        # 结果作为memory传递给目标语言的编码器\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        # src = (batch.size, seq.length)\n",
    "        # src_mask 负责对src加掩码\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "        # 对源语言序列进行编码，得到的结果为\n",
    "        # (batch.size, seq.length, 512)的tensor\n",
    "\n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), \n",
    "          memory, src_mask, tgt_mask)\n",
    "        # 对目标语言序列进行编码，得到的结果为\n",
    "        # (batch.size, seq.length, 512)的tensor\n",
    "\n",
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions.\"\n",
    "    # e.g., size=10\n",
    "    attn_shape = (1, size, size) # (1, 10, 10)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1) \\\n",
    "      .astype('uint8')\n",
    "    # triu: 负责生成一个三角矩阵，k-th对角线以下都是设置为0 \n",
    "    # 上三角中元素为1.\n",
    "    \n",
    "    return torch.from_numpy(subsequent_mask) == 0\n",
    "    # 反转上面的triu得到的上三角矩阵，修改为下三角矩阵。\n",
    "\n",
    "def make_model(src_vocab, tgt_vocab, N=6, \n",
    "               d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    # src_vocab = 源语言词表大小\n",
    "    # tgt_vocab = 目标语言词表大小\n",
    "    \n",
    "    c = copy.deepcopy # 对象的深度copy/clone\n",
    "    attn = MultiHeadedAttention(h, d_model) # 8, 512\n",
    "    # 构造一个MultiHeadAttention对象\n",
    "    \n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    # 512, 2048, 0.1\n",
    "    # 构造一个feed forward对象\n",
    "\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    # 位置编码\n",
    "\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
    "                             c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab))\n",
    "\n",
    "    # This was important from their code. \n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform(p)\n",
    "    return model # EncoderDecoder 对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.layers.0.self_attn.linears.0.weight torch.Size([512, 512])\n",
      "encoder.layers.0.self_attn.linears.0.bias torch.Size([512])\n",
      "encoder.layers.0.self_attn.linears.1.weight torch.Size([512, 512])\n",
      "encoder.layers.0.self_attn.linears.1.bias torch.Size([512])\n",
      "encoder.layers.0.self_attn.linears.2.weight torch.Size([512, 512])\n",
      "encoder.layers.0.self_attn.linears.2.bias torch.Size([512])\n",
      "encoder.layers.0.self_attn.linears.3.weight torch.Size([512, 512])\n",
      "encoder.layers.0.self_attn.linears.3.bias torch.Size([512])\n",
      "encoder.layers.0.feed_forward.w_1.weight torch.Size([2048, 512])\n",
      "encoder.layers.0.feed_forward.w_1.bias torch.Size([2048])\n",
      "encoder.layers.0.feed_forward.w_2.weight torch.Size([512, 2048])\n",
      "encoder.layers.0.feed_forward.w_2.bias torch.Size([512])\n",
      "encoder.layers.0.sublayer.0.norm.a_2 torch.Size([512])\n",
      "encoder.layers.0.sublayer.0.norm.b_2 torch.Size([512])\n",
      "encoder.layers.0.sublayer.1.norm.a_2 torch.Size([512])\n",
      "encoder.layers.0.sublayer.1.norm.b_2 torch.Size([512])\n",
      "encoder.layers.1.self_attn.linears.0.weight torch.Size([512, 512])\n",
      "encoder.layers.1.self_attn.linears.0.bias torch.Size([512])\n",
      "encoder.layers.1.self_attn.linears.1.weight torch.Size([512, 512])\n",
      "encoder.layers.1.self_attn.linears.1.bias torch.Size([512])\n",
      "encoder.layers.1.self_attn.linears.2.weight torch.Size([512, 512])\n",
      "encoder.layers.1.self_attn.linears.2.bias torch.Size([512])\n",
      "encoder.layers.1.self_attn.linears.3.weight torch.Size([512, 512])\n",
      "encoder.layers.1.self_attn.linears.3.bias torch.Size([512])\n",
      "encoder.layers.1.feed_forward.w_1.weight torch.Size([2048, 512])\n",
      "encoder.layers.1.feed_forward.w_1.bias torch.Size([2048])\n",
      "encoder.layers.1.feed_forward.w_2.weight torch.Size([512, 2048])\n",
      "encoder.layers.1.feed_forward.w_2.bias torch.Size([512])\n",
      "encoder.layers.1.sublayer.0.norm.a_2 torch.Size([512])\n",
      "encoder.layers.1.sublayer.0.norm.b_2 torch.Size([512])\n",
      "encoder.layers.1.sublayer.1.norm.a_2 torch.Size([512])\n",
      "encoder.layers.1.sublayer.1.norm.b_2 torch.Size([512])\n",
      "encoder.norm.a_2 torch.Size([512])\n",
      "encoder.norm.b_2 torch.Size([512])\n",
      "decoder.layers.0.self_attn.linears.0.weight torch.Size([512, 512])\n",
      "decoder.layers.0.self_attn.linears.0.bias torch.Size([512])\n",
      "decoder.layers.0.self_attn.linears.1.weight torch.Size([512, 512])\n",
      "decoder.layers.0.self_attn.linears.1.bias torch.Size([512])\n",
      "decoder.layers.0.self_attn.linears.2.weight torch.Size([512, 512])\n",
      "decoder.layers.0.self_attn.linears.2.bias torch.Size([512])\n",
      "decoder.layers.0.self_attn.linears.3.weight torch.Size([512, 512])\n",
      "decoder.layers.0.self_attn.linears.3.bias torch.Size([512])\n",
      "decoder.layers.0.src_attn.linears.0.weight torch.Size([512, 512])\n",
      "decoder.layers.0.src_attn.linears.0.bias torch.Size([512])\n",
      "decoder.layers.0.src_attn.linears.1.weight torch.Size([512, 512])\n",
      "decoder.layers.0.src_attn.linears.1.bias torch.Size([512])\n",
      "decoder.layers.0.src_attn.linears.2.weight torch.Size([512, 512])\n",
      "decoder.layers.0.src_attn.linears.2.bias torch.Size([512])\n",
      "decoder.layers.0.src_attn.linears.3.weight torch.Size([512, 512])\n",
      "decoder.layers.0.src_attn.linears.3.bias torch.Size([512])\n",
      "decoder.layers.0.feed_forward.w_1.weight torch.Size([2048, 512])\n",
      "decoder.layers.0.feed_forward.w_1.bias torch.Size([2048])\n",
      "decoder.layers.0.feed_forward.w_2.weight torch.Size([512, 2048])\n",
      "decoder.layers.0.feed_forward.w_2.bias torch.Size([512])\n",
      "decoder.layers.0.sublayer.0.norm.a_2 torch.Size([512])\n",
      "decoder.layers.0.sublayer.0.norm.b_2 torch.Size([512])\n",
      "decoder.layers.0.sublayer.1.norm.a_2 torch.Size([512])\n",
      "decoder.layers.0.sublayer.1.norm.b_2 torch.Size([512])\n",
      "decoder.layers.0.sublayer.2.norm.a_2 torch.Size([512])\n",
      "decoder.layers.0.sublayer.2.norm.b_2 torch.Size([512])\n",
      "decoder.layers.1.self_attn.linears.0.weight torch.Size([512, 512])\n",
      "decoder.layers.1.self_attn.linears.0.bias torch.Size([512])\n",
      "decoder.layers.1.self_attn.linears.1.weight torch.Size([512, 512])\n",
      "decoder.layers.1.self_attn.linears.1.bias torch.Size([512])\n",
      "decoder.layers.1.self_attn.linears.2.weight torch.Size([512, 512])\n",
      "decoder.layers.1.self_attn.linears.2.bias torch.Size([512])\n",
      "decoder.layers.1.self_attn.linears.3.weight torch.Size([512, 512])\n",
      "decoder.layers.1.self_attn.linears.3.bias torch.Size([512])\n",
      "decoder.layers.1.src_attn.linears.0.weight torch.Size([512, 512])\n",
      "decoder.layers.1.src_attn.linears.0.bias torch.Size([512])\n",
      "decoder.layers.1.src_attn.linears.1.weight torch.Size([512, 512])\n",
      "decoder.layers.1.src_attn.linears.1.bias torch.Size([512])\n",
      "decoder.layers.1.src_attn.linears.2.weight torch.Size([512, 512])\n",
      "decoder.layers.1.src_attn.linears.2.bias torch.Size([512])\n",
      "decoder.layers.1.src_attn.linears.3.weight torch.Size([512, 512])\n",
      "decoder.layers.1.src_attn.linears.3.bias torch.Size([512])\n",
      "decoder.layers.1.feed_forward.w_1.weight torch.Size([2048, 512])\n",
      "decoder.layers.1.feed_forward.w_1.bias torch.Size([2048])\n",
      "decoder.layers.1.feed_forward.w_2.weight torch.Size([512, 2048])\n",
      "decoder.layers.1.feed_forward.w_2.bias torch.Size([512])\n",
      "decoder.layers.1.sublayer.0.norm.a_2 torch.Size([512])\n",
      "decoder.layers.1.sublayer.0.norm.b_2 torch.Size([512])\n",
      "decoder.layers.1.sublayer.1.norm.a_2 torch.Size([512])\n",
      "decoder.layers.1.sublayer.1.norm.b_2 torch.Size([512])\n",
      "decoder.layers.1.sublayer.2.norm.a_2 torch.Size([512])\n",
      "decoder.layers.1.sublayer.2.norm.b_2 torch.Size([512])\n",
      "decoder.norm.a_2 torch.Size([512])\n",
      "decoder.norm.b_2 torch.Size([512])\n",
      "src_embed.0.lut.weight torch.Size([10, 512])\n",
      "tgt_embed.0.lut.weight torch.Size([10, 512])\n",
      "generator.proj.weight torch.Size([10, 512])\n",
      "generator.proj.bias torch.Size([10])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_391643/560343739.py:427: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(p)\n"
     ]
    }
   ],
   "source": [
    "tmp_model = make_model(10, 10, 2)\n",
    "# src_vocab_size = 10\n",
    "# tgt_vocab_size = 10\n",
    "# N = 2, number for EncoderLayer and DecoderLayer\n",
    "\n",
    "for name, param in tmp_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print (name, param.data.shape)\n",
    "    else:\n",
    "        print ('no gradient necessary', name, param.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"Object for holding a batch of data with mask during training.\"\n",
    "    def __init__(self, src, trg=None, pad=0):\n",
    "        # src: 源语言序列，(batch.size, src.seq.len)\n",
    "        # 二维tensor，第一维度是batch.size；第二个维度是源语言句子的长度\n",
    "        # 例如：[ [2,1,3,4], [2,3,1,4] ]这样的二行四列的，\n",
    "        # 1-4代表每个单词word的id\n",
    "        \n",
    "        # trg: 目标语言序列，默认为空，其shape和src类似\n",
    "        # (batch.size, trg.seq.len)，\n",
    "        #二维tensor，第一维度是batch.size；第二个维度是目标语言句子的长度\n",
    "        # 例如trg=[ [2,1,3,4], [2,3,1,4] ] for a \"copy network\"\n",
    "        # (输出序列和输入序列完全相同）\n",
    "        \n",
    "        # pad: 源语言和目标语言统一使用的 位置填充符号，'<blank>'\n",
    "        # 所对应的id，这里默认为0\n",
    "        # 例如，如果一个source sequence，长度不到4，则在右边补0\n",
    "        # [1,2] -> [1,2,0,0]\n",
    "        \n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        # src = (batch.size, seq.len) -> != pad -> \n",
    "        # (batch.size, seq.len) -> usnqueeze ->\n",
    "        # (batch.size, 1, seq.len) 相当于在倒数第二个维度扩展\n",
    "        # e.g., src=[ [2,1,3,4], [2,3,1,4] ]对应的是\n",
    "        # src_mask=[ [[1,1,1,1], [1,1,1,1]] ]\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:, :-1] # 重要\n",
    "            # trg 相当于目标序列的前N-1个单词的序列\n",
    "            #（去掉了最后一个词）\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            # trg_y 相当于目标序列的后N-1个单词的序列\n",
    "            # (去掉了第一个词）\n",
    "            # 目的是(src + trg) 来预测出来(trg_y)，\n",
    "            # 这个在illustrated transformer中详细图示过。\n",
    "            self.trg_mask = \\\n",
    "                self.make_std_mask(self.trg, pad)\n",
    "            self.ntokens = (self.trg_y != pad).data.sum()\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        # 这里的tgt类似于：\n",
    "        #[ [2,1,3], [2,3,1] ] （最初的输入目标序列，分别去掉了最后一个词\n",
    "        # pad=0, '<blank>'的id编号\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        # 得到的tgt_mask类似于\n",
    "        # tgt_mask = tensor([[[1, 1, 1]],[[1, 1, 1]]], dtype=torch.uint8)\n",
    "        # shape=(2,1,3)\n",
    "        tgt_mask = tgt_mask & Variable(\n",
    "            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "        # 先看subsequent_mask, 其输入的是tgt.size(-1)=3\n",
    "        # 这个函数的输出为= tensor([[[1, 0, 0],\n",
    "        # [1, 1, 0],\n",
    "        # [1, 1, 1]]], dtype=torch.uint8)\n",
    "        # type_as 把这个tensor转成tgt_mask.data的type(也是torch.uint8)\n",
    "        \n",
    "        # 这样的话，&的两边的tensor分别是(2,1,3), (1,3,3);\n",
    "        #tgt_mask = tensor([[[1, 1, 1]],[[1, 1, 1]]], dtype=torch.uint8)\n",
    "        #and\n",
    "        # tensor([[[1, 0, 0], [1, 1, 0], [1, 1, 1]]], dtype=torch.uint8)\n",
    "        \n",
    "        # (2,3,3)就是得到的tensor\n",
    "        # tgt_mask.data = tensor([[[1, 0, 0],\n",
    "        # [1, 1, 0],\n",
    "        # [1, 1, 1]],\n",
    "\n",
    "        #[[1, 0, 0],\n",
    "        # [1, 1, 0],\n",
    "        # [1, 1, 1]]], dtype=torch.uint8)\n",
    "        return tgt_mask\n",
    "    \n",
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # optimizer = Adam (Parameter Group 0\n",
    "        #    amsgrad: False\n",
    "        #    betas: (0.9, 0.98)\n",
    "        #    eps: 1e-09\n",
    "        #    lr: 0\n",
    "        #    weight_decay: 0\n",
    "        #)\n",
    "        self._step = 0\n",
    "        self.warmup = warmup # e.g., 4000 轮 热身\n",
    "        self.factor = factor # e.g., 2\n",
    "        self.model_size = model_size # 512\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate`(learning rate) above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup ** (-1.5))) # 当step=Warmup时，学习率开始下降，即预热Warmup步\n",
    "        \n",
    "def get_std_opt(model):\n",
    "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
    "            torch.optim.Adam(model.parameters(), lr=0, \n",
    "            betas=(0.9, 0.98), eps=1e-9))\n",
    "\n",
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False)\n",
    "        self.padding_idx = padding_idx # '<blank>' 的id\n",
    "        self.confidence = 1.0 - smoothing # 自留的概率值、得分 e.g. 0.6\n",
    "        self.smoothing = smoothing # 均分出去的概率值，得分 e.g. 0.4\n",
    "        self.size = size # target vocab size 目标语言词表大小\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        \"in real-world case: 真实情况下\"\n",
    "        #  x的shape为(batch.size * seq.len, target.vocab.size)\n",
    "        # y的shape是(batch.size * seq.len)\n",
    "        \n",
    "        # x=logits，(seq.len, target.vocab.size)\n",
    "        # 每一行，代表一个位置的词\n",
    "        # 类似于：假设seq.len=3, target.vocab.size=5\n",
    "        # x中保存的是log(prob)\n",
    "        #x = tensor([[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233],\n",
    "        #[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233],\n",
    "        #[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233]])\n",
    "        \n",
    "        # target 类似于：\n",
    "        # target = tensor([2, 1, 0])，torch.size=(3)\n",
    "        \n",
    "        assert x.size(1) == self.size # 目标语言词表大小\n",
    "        true_dist = x.data.clone()\n",
    "        # true_dist = tensor([[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233],\n",
    "        #[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233],\n",
    "        #[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233]])\n",
    "        \n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        # true_dist = tensor([[0.1333, 0.1333, 0.1333, 0.1333, 0.1333],\n",
    "        #[0.1333, 0.1333, 0.1333, 0.1333, 0.1333],\n",
    "        #[0.1333, 0.1333, 0.1333, 0.1333, 0.1333]])\n",
    "        \n",
    "        # 注意，这里分母target.vocab.size-2是因为\n",
    "        # (1) 最优值 0.6要占一个位置；\n",
    "        # (2) 填充词 <blank> 要被排除在外\n",
    "        # 所以被激活的目标语言词表大小就是self.size-2\n",
    "        \n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), \n",
    "          self.confidence)\n",
    "        # target.data.unsqueeze(1) -> \n",
    "        # tensor([[2],\n",
    "        #[1],\n",
    "        #[0]]); shape=torch.Size([3, 1])  \n",
    "        # self.confidence = 0.6\n",
    "        \n",
    "        # 根据target.data的指示，按照列优先(1)的原则，把0.6这个值\n",
    "        # 填入true_dist: 因为target.data是2,1,0的内容，\n",
    "        # 所以，0.6填入第0行的第2列（列号，行号都是0开始）\n",
    "        # 0.6填入第1行的第1列\n",
    "        # 0.6填入第2行的第0列：\n",
    "        # true_dist = tensor([[0.1333, 0.1333, 0.6000, 0.1333, 0.1333],\n",
    "        #[0.1333, 0.6000, 0.1333, 0.1333, 0.1333],\n",
    "        #[0.6000, 0.1333, 0.1333, 0.1333, 0.1333]])\n",
    "          \n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        # true_dist = tensor([[0.0000, 0.1333, 0.6000, 0.1333, 0.1333],\n",
    "        #[0.0000, 0.6000, 0.1333, 0.1333, 0.1333],\n",
    "        #[0.0000, 0.1333, 0.1333, 0.1333, 0.1333]])\n",
    "        # 设置true_dist这个tensor的第一列的值全为0\n",
    "        # 因为这个是填充词'<blank>'所在的id位置，不应该计入\n",
    "        # 目标词表。需要注意的是，true_dist的每一列，代表目标语言词表\n",
    "        #中的一个词的id\n",
    "        \n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        # mask = tensor([[2]]), 也就是说，最后一个词 2,1,0中的0，\n",
    "        # 因为是'<blank>'的id，所以通过上面的一步，把他们找出来\n",
    "        \n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "            # 当target reference序列中有0这个'<blank>'的时候，则需要把\n",
    "            # 这一行的值都清空。\n",
    "            # 在一个batch里面的时候，可能两个序列长度不一，所以短的序列需要\n",
    "            # pad '<blank>'来填充，所以会出现类似于(2,1,0)这样的情况\n",
    "            # true_dist = tensor([[0.0000, 0.1333, 0.6000, 0.1333, 0.1333],\n",
    "            # [0.0000, 0.6000, 0.1333, 0.1333, 0.1333],\n",
    "            # [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, \n",
    "          Variable(true_dist, requires_grad=False))\n",
    "          # 这一步就是调用KL loss来计算\n",
    "          # x = tensor([[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233],\n",
    "          #[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233],\n",
    "          #[-20.7233,  -1.6094,  -0.3567,  -2.3026, -20.7233]])\n",
    "          \n",
    "          # true_dist=tensor([[0.0000, 0.1333, 0.6000, 0.1333, 0.1333],\n",
    "          # [0.0000, 0.6000, 0.1333, 0.1333, 0.1333],\n",
    "          # [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
    "          # 之间的loss了。细节可以参考我的那篇illustrated transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(V, batch, nbatches):\n",
    "    \"Generate random data for a src-tgt copy task.\"\n",
    "    for i in range(nbatches):\n",
    "        data = torch.from_numpy \\\n",
    "            (np.random.randint(1, V, size=(batch, 10)))\n",
    "        data[:, 0] = 1\n",
    "        src = Variable(data, requires_grad=False)\n",
    "        tgt = Variable(data, requires_grad=False)\n",
    "        yield Batch(src, tgt, 0)\n",
    "\n",
    "class SimpleLossCompute:\n",
    "    \"A simple loss compute and train function.\"\n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        self.generator = generator # Generator 对象, linear+softmax\n",
    "        self.criterion = criterion # LabelSmooth对象，计算loss\n",
    "        self.opt = opt # NormOpt对象，优化算法对象\n",
    "        \n",
    "    def __call__(self, x, y, norm):\n",
    "        # e.g., x为(2,3,8), batch.size=2, seq.len=3, d_model=8\n",
    "        # y = tensor([[4, 2, 1],\n",
    "        #[4, 4, 4]], dtype=torch.int32)\n",
    "        \n",
    "        # norm: (y=trg_y中非'<blank>'的token的个数)\n",
    "        \"attention here\"\n",
    "        \n",
    "        x = self.generator(x)\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), \n",
    "                              y.contiguous().view(-1)) / norm.item()\n",
    "        # 变形后，x类似于(batch.size*seq.len, target.vocab.size)\n",
    "        # y为(target.vocab.size)\n",
    "        # 然后调用LabelSmooth来计算loss\n",
    "        loss.backward()\n",
    "        if self.opt is not None:\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        #return loss.data[0] * norm\n",
    "        \"attention here\"\n",
    "        return loss.data.item() * norm.item()\n",
    "    \n",
    "def run_epoch(aepoch, data_iter, model, loss_compute):\n",
    "\n",
    "    \"Standard Training and Logging Function\"\n",
    "    # data_iter = 所有数据的打包\n",
    "    # model = EncoderDecoder 对象\n",
    "    # loss_compute = SimpleLossCompute对象\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        # 对每个batch循环\n",
    "        out = model.forward(batch.src, batch.trg, \n",
    "                            batch.src_mask, batch.trg_mask)\n",
    "        # 使用目前的model，对batch.src+batch.trg进行forward\n",
    "                            \n",
    "        # e.g.,\n",
    "        # batch.src (2,4) = tensor([[1, 4, 2, 1],\n",
    "        # [1, 4, 4, 4]], dtype=torch.int32)\n",
    "        \n",
    "        # batch.trg (2,3) = tensor([[1, 4, 2],\n",
    "        # [1, 4, 4]], dtype=torch.int32)\n",
    "        \n",
    "        # batch.src_mask (2,1,4) = tensor([[[1, 1, 1, 1]],\n",
    "        # [[1, 1, 1, 1]]], dtype=torch.uint8)\n",
    "        \n",
    "        # batch.trg_mask (2,3,3) = tensor([[[1, 0, 0],\n",
    "        # [1, 1, 0],\n",
    "        # [1, 1, 1]],\n",
    "\n",
    "        #[[1, 0, 0],\n",
    "        # [1, 1, 0],\n",
    "         #[1, 1, 1]]], dtype=torch.uint8)\n",
    "         \n",
    "        # and out (2,3,8):\n",
    "        # out = tensor([[[-0.4749, -0.4887,  0.1245, -0.4042,  0.5301,  \n",
    "        #   1.7662, -1.6224, 0.5694],\n",
    "        # [ 0.4683, -0.7813,  0.2845,  0.4464, -0.3088, -0.1751, -1.6643,\n",
    "        #   1.7303],\n",
    "         #[-1.1600, -0.2348,  1.0631,  1.3192, -0.9453,  0.3538,  0.7051...                 \n",
    "        \n",
    "        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n",
    "        # out和trg_y计算Loss\n",
    "        # ntokens = 6 (trg_y中非'<blank>'的token的个数)\n",
    "        # 注意，这里是token,不是unique word\n",
    "        # 例如[ [ [1, 2, 3], [2,3,4] ]中有6个token,而只有4个unique word\n",
    "        \n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "        if i % 50 == 1:\n",
    "            elapsed = time.time() - start\n",
    "            \"attention here 这里隐藏一个bug\"\n",
    "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
    "                   (i, loss / batch.ntokens, tokens / elapsed))\n",
    "            # print ('epoch step: {}:{} Loss: {}/{}, tokens per sec: {}/{}'\n",
    "            #         .format(aepoch, i, loss, batch.ntokens, \n",
    "            #         tokens, elapsed))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "    return total_loss / total_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_391643/560343739.py:427: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(p)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start model training...\n",
      "epoch=0, training...\n",
      "Epoch Step: 1 Loss: 1.846526 Tokens per Sec: 1058.517944\n",
      "evaluating...\n",
      "Epoch Step: 1 Loss: 2.040799 Tokens per Sec: 1688.188354\n",
      "tensor(2.0050)\n",
      "epoch=1, training...\n",
      "Epoch Step: 1 Loss: 2.255836 Tokens per Sec: 1198.453369\n",
      "evaluating...\n",
      "Epoch Step: 1 Loss: 1.990043 Tokens per Sec: 1483.535156\n",
      "tensor(2.0521)\n",
      "epoch=2, training...\n",
      "Epoch Step: 1 Loss: 2.159793 Tokens per Sec: 1025.143677\n",
      "evaluating...\n",
      "Epoch Step: 1 Loss: 2.163441 Tokens per Sec: 1464.363770\n",
      "tensor(2.0057)\n",
      "epoch=3, training...\n",
      "Epoch Step: 1 Loss: 1.817395 Tokens per Sec: 1025.436035\n",
      "evaluating...\n",
      "Epoch Step: 1 Loss: 1.588890 Tokens per Sec: 1460.581787\n",
      "tensor(1.7241)\n",
      "epoch=4, training...\n",
      "Epoch Step: 1 Loss: 1.875162 Tokens per Sec: 1024.569702\n",
      "evaluating...\n",
      "Epoch Step: 1 Loss: 1.923357 Tokens per Sec: 1475.676880\n",
      "tensor(1.9058)\n",
      "epoch=5, training...\n",
      "Epoch Step: 1 Loss: 1.833811 Tokens per Sec: 1027.791748\n",
      "evaluating...\n",
      "Epoch Step: 1 Loss: 1.961811 Tokens per Sec: 1447.885864\n",
      "tensor(1.8873)\n",
      "epoch=6, training...\n",
      "Epoch Step: 1 Loss: 2.044039 Tokens per Sec: 1027.068237\n",
      "evaluating...\n",
      "Epoch Step: 1 Loss: 1.648770 Tokens per Sec: 1457.107422\n",
      "tensor(1.7342)\n",
      "epoch=7, training...\n",
      "Epoch Step: 1 Loss: 1.801462 Tokens per Sec: 1026.739990\n",
      "evaluating...\n",
      "Epoch Step: 1 Loss: 1.846478 Tokens per Sec: 1593.008911\n",
      "tensor(1.7448)\n",
      "epoch=8, training...\n",
      "Epoch Step: 1 Loss: 1.871770 Tokens per Sec: 1202.351807\n",
      "evaluating...\n",
      "Epoch Step: 1 Loss: 1.638483 Tokens per Sec: 1701.341919\n",
      "tensor(1.6284)\n",
      "epoch=9, training...\n",
      "Epoch Step: 1 Loss: 1.702315 Tokens per Sec: 1205.028931\n",
      "evaluating...\n",
      "Epoch Step: 1 Loss: 1.531200 Tokens per Sec: 1677.824097\n",
      "tensor(1.6693)\n",
      "memory=tensor([[[-0.7692, -0.2757, -1.3214,  0.1722, -0.8793,  0.6044,  1.7321,\n",
      "           0.7264],\n",
      "         [ 0.3349,  0.1445, -1.4886,  0.2449, -0.6423, -0.5596,  2.0044,\n",
      "          -0.0391],\n",
      "         [ 0.5812,  0.3889, -1.6545,  0.9503, -0.3005, -0.7457,  1.3952,\n",
      "          -0.6208],\n",
      "         [ 0.9051,  0.5053, -1.8776,  1.2736,  0.0958, -0.5315,  0.3032,\n",
      "          -0.6860]]], grad_fn=<AddBackward0>), memory.shape=torch.Size([1, 4, 8])\n",
      "ys=tensor([[1]]), ys.shape=torch.Size([1, 1])\n",
      "out=tensor([[[-0.1700, -0.9307,  1.2741,  1.2617, -0.4845, -1.0569,  0.9117,\n",
      "          -0.8014]]], grad_fn=<AddBackward0>), out.shape=torch.Size([1, 1, 8])\n",
      "next_word=tensor([1]), next_word.shape=torch.Size([1])\n",
      "out=tensor([[[-0.1700, -0.9307,  1.2741,  1.2617, -0.4845, -1.0569,  0.9117,\n",
      "          -0.8014],\n",
      "         [ 0.2658, -1.2778,  1.2795,  1.4348, -0.2494, -0.9221,  0.2215,\n",
      "          -0.7481]]], grad_fn=<AddBackward0>), out.shape=torch.Size([1, 2, 8])\n",
      "next_word=tensor([2]), next_word.shape=torch.Size([1])\n",
      "out=tensor([[[-0.1700, -0.9307,  1.2741,  1.2617, -0.4845, -1.0569,  0.9117,\n",
      "          -0.8014],\n",
      "         [ 0.2658, -1.2778,  1.2795,  1.4348, -0.2494, -0.9221,  0.2215,\n",
      "          -0.7481],\n",
      "         [ 0.7450, -1.1711,  0.4404,  0.9617,  0.7732, -0.3826,  0.3727,\n",
      "          -1.7361]]], grad_fn=<AddBackward0>), out.shape=torch.Size([1, 3, 8])\n",
      "next_word=tensor([2]), next_word.shape=torch.Size([1])\n",
      "out=tensor([[[-0.1700, -0.9307,  1.2741,  1.2617, -0.4845, -1.0569,  0.9117,\n",
      "          -0.8014],\n",
      "         [ 0.2658, -1.2778,  1.2795,  1.4348, -0.2494, -0.9221,  0.2215,\n",
      "          -0.7481],\n",
      "         [ 0.7450, -1.1711,  0.4404,  0.9617,  0.7732, -0.3826,  0.3727,\n",
      "          -1.7361],\n",
      "         [ 0.2735, -1.1959,  0.4039,  1.1802,  0.6347, -0.0640,  0.5512,\n",
      "          -1.7797]]], grad_fn=<AddBackward0>), out.shape=torch.Size([1, 4, 8])\n",
      "next_word=tensor([3]), next_word.shape=torch.Size([1])\n",
      "tensor([[1, 1, 2, 2, 3]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the simple copy task.\n",
    "V = 5 # here V is the vocab size of source and target languages (sequences)\n",
    "criterion = LabelSmoothing(size=V, \n",
    "    padding_idx=0, smoothing=0.01) # 创建损失函数计算对象\n",
    "    \n",
    "model = make_model(V, V, N=2, d_model=8, d_ff=16, h=2) \n",
    "# EncoderDecoder对象构造\n",
    "'''\n",
    "in make_model: src_vocab_size=11, tgt_vocab_size=11, \n",
    "    N=2, d_model=512, d_ff=2048, h=8, dropout=0.1\n",
    "'''\n",
    "\n",
    "model_opt = NoamOpt(model.src_embed[0].d_model, 1, 400,\n",
    "        torch.optim.Adam(model.parameters(), \n",
    "        lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "# 模型最优化算法的对象\n",
    "\n",
    "lossfun = SimpleLossCompute(model.generator, \n",
    "        criterion, model_opt)\n",
    "if True:\n",
    "    print ('start model training...')\n",
    "    for epoch in range(10):\n",
    "        print ('epoch={}, training...'.format(epoch))\n",
    "        model.train() # set the model into \"train\" mode\n",
    "        # 设置模型进入训练模式\n",
    "        \n",
    "        #lossfun = SimpleLossCompute(model.generator, \n",
    "        #    criterion, model_opt) # 不需要在这里定义lossfun\n",
    "        \n",
    "        run_epoch(epoch, data_gen(V, 4, 2), model, lossfun)\n",
    "        # 重新构造一批数据，并执行训练\n",
    "        \n",
    "        model.eval() # 模型进入evaluation模式 (dropout，反向传播无效）\n",
    "        print ('evaluating...')\n",
    "        print(run_epoch(epoch, data_gen(V, 4, 2), model, \n",
    "                        SimpleLossCompute(model.generator, \n",
    "                        criterion, None)))\n",
    "        # 这里None表示优化函数为None，所以不进行参数更新 \n",
    "\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    memory = model.encode(src, src_mask) \n",
    "    # 源语言的一个batch\n",
    "    # 执行encode编码工作，得到memory \n",
    "    # shape=(batch.size, src.seq.len, d_model)\n",
    "    \n",
    "    # src = (1,4), batch.size=1, seq.len=4\n",
    "    # src_mask = (1,1,4) with all ones\n",
    "    # start_symbol=1\n",
    "    \n",
    "    print ('memory={}, memory.shape={}'.format(memory, \n",
    "        memory.shape))\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    # 最初ys=[[1]], size=(1,1); 这里start_symbol=1\n",
    "    print ('ys={}, ys.shape={}'.format(ys, ys.shape))\n",
    "    for i in range(max_len-1): # max_len = 5\n",
    "        out = model.decode(memory, src_mask, \n",
    "                           Variable(ys), \n",
    "                           Variable(subsequent_mask(ys.size(1))\n",
    "                                    .type_as(src.data)))\n",
    "        # memory, (1, 4, 8), 1=batch.size, 4=src.seq.len, 8=d_model\n",
    "        # src_mask = (1,1,4) with all ones\n",
    "        # out, (1, 1, 8), 1=batch.size, 1=seq.len, 8=d_model                             \n",
    "        print ('out={}, out.shape={}'.format(out, out.shape))\n",
    "        prob = model.generator(out[:, -1]) \n",
    "        # pick the right-most word\n",
    "        # (1=batch.size,8) -> generator -> prob=(1,5) 5=trg.vocab.size\n",
    "        # -1 for ? only look at the final (out) word's vector\n",
    "        _, next_word = torch.max(prob, dim = 1)\n",
    "        print ('next_word={}, next_word.shape={}'.format(next_word, next_word.shape))\n",
    "        next_word = next_word.data[0]\n",
    "        # word id of \"next_word\"\n",
    "        ys = torch.cat([ys, \n",
    "          torch.ones(1, 1).type_as(src.data).fill_(next_word)], \n",
    "          dim=1)\n",
    "        # ys is in shape of (1,2) now, i.e., 2 words in current seq\n",
    "    return ys\n",
    "\n",
    "if True:\n",
    "    model.eval()\n",
    "    src = Variable(torch.LongTensor([[1,2,3,4]]))\n",
    "    src_mask = Variable(torch.ones(1, 1, 4))\n",
    "    print(greedy_decode(model, src, src_mask, max_len=5, \n",
    "        start_symbol=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LargeModel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
